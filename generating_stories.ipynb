{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import random\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pdd\n",
    "\n",
    "# Presentation generation\n",
    "from pptx import Presentation\n",
    "from pptx.util import Pt, Inches\n",
    "from pptx.enum.shapes import MSO_SHAPE\n",
    "from PIL import Image, ImageFont\n",
    "\n",
    "# Partitioning\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "\n",
    "# Summarisation\n",
    "from transformers import pipeline\n",
    "\n",
    "# Alignment\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Prompt generation\n",
    "from g4f.client import Client\n",
    "\n",
    "# PKL File creation (w/ encoding)\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Image generation\n",
    "from diffusers import AnimateDiffPipeline, DDIMScheduler, MotionAdapter, StableDiffusionPipeline\n",
    "from diffusers.utils import export_to_gif\n",
    "\n",
    "# DOC2PPT models\n",
    "import torch as T\n",
    "import torch.nn as nn\n",
    "from dataset import *\n",
    "from model import *\n",
    "from mlp_layout import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Presentation helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap text to fit within a given width for a PowerPoint TextFrame.\n",
    "def wrap_text(text, max_width, text_frame, font_size=18, font_path='calibri.ttf'):\n",
    "    # Initialize PIL font\n",
    "    font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "    # Get the width of the text when rendered with the given font.\n",
    "    def get_text_width(text):\n",
    "        bbox = font.getbbox(text)\n",
    "        return bbox[2] - bbox[0]\n",
    "    \n",
    "    # Function to add a paragraph with text and formatting\n",
    "    def add_paragraph(text, font_size, text_frame):\n",
    "        paragraph = text_frame.add_paragraph()\n",
    "        paragraph.text = text\n",
    "        paragraph.font.size = Pt(font_size)\n",
    "        return paragraph\n",
    "\n",
    "    # Add extra newlines between sentences\n",
    "    sentences_ = text.split('. ')\n",
    "    sentences = [sentence + '.' for sentence in sentences_[:-1] if sentence]  # Re-add the period\n",
    "    sentences.append(sentences_[-1])\n",
    "    paragraphs = [\" \".join(sentences)]\n",
    "        \n",
    "    for paragraph_text in paragraphs:\n",
    "        # Add paragraph to text frame\n",
    "        paragraph = add_paragraph(paragraph_text, font_size, text_frame)\n",
    "\n",
    "        # Split the paragraph text into words\n",
    "        words = paragraph_text.split()\n",
    "        current_line = []\n",
    "\n",
    "        for word in words:\n",
    "            # Create a copy of the current line with the new word\n",
    "            current_line_copy = current_line + [word]\n",
    "            line_text = ' '.join(current_line_copy)\n",
    "            \n",
    "            # Measure the width of the line\n",
    "            text_width = get_text_width(line_text)\n",
    "            \n",
    "            # Convert max_width to pixels (Pillow uses pixels)\n",
    "            max_width_pixels = max_width.pt\n",
    "            \n",
    "            if text_width <= max_width_pixels:\n",
    "                # If it fits, update the current line\n",
    "                current_line = current_line_copy\n",
    "            else:\n",
    "                # If it doesn't fit, finalize the current line and start a new line\n",
    "                paragraph.text = ' '.join(current_line)\n",
    "                current_line = [word]\n",
    "                paragraph = add_paragraph(' '.join([word]), font_size, text_frame)\n",
    "                current_line = []\n",
    "\n",
    "        # Set the last line if there is any\n",
    "        if current_line:\n",
    "            paragraph.text = ' '.join(current_line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Ground-truth Slide Decks with BART-summarisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Partitioning of Paragraphs within Chapters\n",
    "- chapters == paper\n",
    "- partitions == section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get files from github\n",
    "with open('./build_dataset/chapter_summary_aligned_train_split.jsonl.gathered') as fd:\n",
    "    book = [json.loads(line) for line in fd]\n",
    "\n",
    "def get_partitions(text):\n",
    "  # Add the first chapter and its text\n",
    "        super_list = text\n",
    "\n",
    "        # Partition base on number of words = 1024 tokens\n",
    "        partitions = []\n",
    "        counter = 0\n",
    "\n",
    "        section = []\n",
    "        for paragraph in super_list:\n",
    "          tmp_count = counter + len(paragraph.split(\" \"))\n",
    "          # If counter is over 1024, append section and reset counter and section\n",
    "          if tmp_count > 512:\n",
    "            partitions.append(section)\n",
    "            counter = 0\n",
    "            section = []\n",
    "          # Otherwise, update counter and add paragraph to section\n",
    "          counter += len(paragraph.split(\" \"))\n",
    "          section.append(paragraph)\n",
    "\n",
    "        # For last section\n",
    "        if section != []:\n",
    "          partitions.append(section)\n",
    "        return partitions\n",
    "\n",
    "books = {}\n",
    "chapter_list = {}\n",
    "chosen_books = ['The Hound of the Baskervilles', 'Frankenstein', 'The Goose Girl', 'A Christmas Carol', 'Wuthering Heights', 'A Tale of Two Cities', 'Little Women', 'Candide', 'The Turn of the Screw', 'Treasure Island']\n",
    "\n",
    "for ix, example in enumerate(book):\n",
    "    b_list = example['book_id'].split('.')\n",
    "    if b_list[0] in chosen_books:\n",
    "        if len(b_list) == 2:\n",
    "            book, chap  = b_list\n",
    "        # If book has volumes - consider it as part of title (title+volume)\n",
    "        elif len(b_list) == 3:\n",
    "            book, chap = b_list[0] + \".\" + b_list[1], b_list[2]\n",
    "        if book not in books:\n",
    "            books[book] = {}\n",
    "            books[book][chap] = get_partitions(example['text'])\n",
    "\n",
    "            # Initialise chapter\n",
    "            chapter_list[book] = []\n",
    "\n",
    "        if chap not in chapter_list:\n",
    "            chapter_list[book].append(chap)\n",
    "\n",
    "        if chap not in books[book]:\n",
    "            books[book][chap] = get_partitions(example['text'])\n",
    "\n",
    "with open('./build_dataset/book_dataset.pkl', 'wb') as file:\n",
    "    pickle.dump(books, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "books = pickle.load(open('./build_dataset/book_dataset.pkl', 'rb'))\n",
    "\n",
    "book_sum = {}\n",
    "for book in tqdm(books, desc=f\"Summarising books\"):\n",
    "    title_check = book.split(\".\")\n",
    "    if title_check[0] in chosen_books:\n",
    "        chap_sum = {}\n",
    "        for chap in tqdm(books[book], desc=f\"Summarising Chapters in {book}\", leave=False):\n",
    "            section = books[book][chap]\n",
    "            summaries = []\n",
    "            for par in section:\n",
    "                s = \" \".join(par)\n",
    "                ARTICLE = f\"\"\"{s}\"\"\"\n",
    "\n",
    "                try:\n",
    "                    out = summarizer(ARTICLE, max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "                    summaries.append(out)\n",
    "                except:\n",
    "                    print(\"ERROR!\")\n",
    "            chap_sum[chap] = summaries\n",
    "        book_sum[book] = chap_sum   \n",
    "\n",
    "with open('./build_dataset/book_summaries.pkl', 'wb') as file:\n",
    "    pickle.dump(book_sum, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aligning Raw Text with Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing books...: 100%|████████████████████████| 1/1 [00:30<00:00, 30.72s/it]\n"
     ]
    }
   ],
   "source": [
    "books = pickle.load(open('./build_dataset/book_dataset.pkl', 'rb'))\n",
    "summ = pickle.load(open('./build_dataset/book_summaries.pkl', 'rb'))\n",
    "\n",
    "# Load a pre-trained sentence transformer model\n",
    "model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "\n",
    "book_labels = {}\n",
    "for title in tqdm(books, desc=\"Processing books...\"):\n",
    "    chap_labels = []\n",
    "    for chapter in books[title]:\n",
    "        section_labels = []\n",
    "        for summ_sen, paragraphs in zip(summ[title][chapter], books[title][chapter]):\n",
    "            if summ_sen and paragraphs:\n",
    "                # Generate embeddings\n",
    "                sentence_embedding = model.encode(summ_sen)\n",
    "                sentence_list_embeddings = model.encode(paragraphs)\n",
    "\n",
    "                # Compute cosine similarities\n",
    "                cosine_similarities = util.cos_sim(sentence_embedding, sentence_list_embeddings).flatten()\n",
    "\n",
    "                # Find the index of the best match\n",
    "                best_match_index = cosine_similarities.argmax().item()\n",
    "                best_match_sentence = paragraphs[best_match_index]\n",
    "\n",
    "                section_labels.append(best_match_index)\n",
    "            else:\n",
    "                section_labels.append(None)\n",
    "        chap_labels.append(section_labels)\n",
    "    book_labels[title] = chap_labels\n",
    "\n",
    "with open('./build_dataset/book_labels.pkl', 'wb') as file:\n",
    "    pickle.dump(book_labels, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Presentation-making (Ground Truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for gifs\n",
    "slide_dir = './groundtruth_slides'\n",
    "if not os.path.exists(slide_dir):\n",
    "    os.makedirs(slide_dir)\n",
    "\n",
    "def normalise_layout(left, top, width, height, slide_width, slide_height):\n",
    "    normalized_bbox = [\n",
    "        left / slide_width,   # x_min\n",
    "        top / slide_height,  # y_min\n",
    "        width / slide_width,   # x_max\n",
    "        height / slide_height   # y_max\n",
    "    ]\n",
    "    return normalized_bbox # divided by 914400 to convert to inches\n",
    "\n",
    "summaries = pickle.load(open('./build_dataset/book_summaries.pkl', 'rb'))\n",
    "type_scene = [title.lower().replace(\" \", \"_\") for title in summaries.keys()]\n",
    "predefined_layouts = [{\"text\": [0.25, 5.25, 9.5, 1], \"image\": [2.5, 0.1, 5, 5]}, {\"text\": [0.25, 0.1, 9.5, 1], \"image\": [2.5, 2.25, 5, 5]}, {\"text\": [5.25, 1, 4.5, 5], \"image\": [0.1, 1, 5, 5]}, {\"text\": [0.1, 1, 4.5, 5], \"image\": [4.85, 1, 5, 5]}]\n",
    "\n",
    "gd_bbox = {}\n",
    "for bk_title, gifs in zip(summaries, type_scene):\n",
    "    chapters = summaries[bk_title]\n",
    "    chap_bbox = {}\n",
    "    for ch in chapters:\n",
    "        # Create a presentation object\n",
    "        prs = Presentation()\n",
    "        text = summaries[bk_title][ch]\n",
    "\n",
    "        slide_objs = []\n",
    "        for sentence in text:\n",
    "            layout = random.choice([0, 1, 2, 3])\n",
    "            # Image bounding box \n",
    "            i_left, i_top, i_width, i_height = [Inches(pt) for pt in predefined_layouts[layout]['image']]\n",
    "            # Text bounding box \n",
    "            t_left, t_top, t_width, t_height = [Inches(pt) for pt in predefined_layouts[layout]['text']]\n",
    "\n",
    "            # Add a slide with a title and content layout\n",
    "            slide_layout = prs.slide_layouts[6]\n",
    "            slide = prs.slides.add_slide(slide_layout)\n",
    "            # Get the slide width and height \n",
    "            slide_width = prs.slide_width \n",
    "            slide_height = prs.slide_height\n",
    "\n",
    "            # BBOX\n",
    "            slide_object = {'text': normalise_layout(t_left, t_top, t_width, t_height, slide_width, slide_height), 'image': normalise_layout(i_left, i_top, i_width, i_height, slide_width, slide_height)}\n",
    "            slide_objs.append(slide_object)\n",
    "\n",
    "            # Add placeholder images (plain rectangle)\n",
    "            slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, left=i_left, top=i_top, width=i_width, height=i_height)\n",
    "\n",
    "            # Add text\n",
    "            text_box = slide.shapes.add_textbox(left=t_left, top=t_top, width=t_width, height=t_height)\n",
    "            text_frame = text_box.text_frame\n",
    "\n",
    "            wrap_text(text=sentence, max_width=t_width, text_frame=text_frame, layout=layout)\n",
    "        chap_bbox[ch] = slide_objs\n",
    "        prs.save(f'./groundtruth_slides/{gifs}_{ch}.pptx')\n",
    "    gd_bbox[bk_title] = chap_bbox\n",
    "\n",
    "with open('./build_dataset/book_bbox.pkl', 'wb') as file:\n",
    "    pickle.dump(gd_bbox, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper-slide Pair PKL File Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained ResNet-152 model for image embeddings\n",
    "img_model = models.resnet152(weights='IMAGENET1K_V1')\n",
    "# Remove the final fully connected layer\n",
    "img_model = T.nn.Sequential(*(list(img_model.children())[:-1]))\n",
    "img_model.eval()\n",
    "\n",
    "# Placeholder image\n",
    "image_array = np.zeros((480, 480, 3), dtype=np.uint8)  # Replace this with your actual image array\n",
    "# Convert the NumPy array to a Pytorch tensor\n",
    "image_tensor = T.from_numpy(image_array).permute(2, 0, 1).float()\n",
    "# Normalize the pixel values to [0, 1]\n",
    "image_tensor /= 255.0\n",
    "# Define the normalization transform (ImageNet normalization)\n",
    "normalize = transforms.Normalize(\n",
    "    mean=[0.485, 0.456, 0.406],\n",
    "    std=[0.229, 0.224, 0.225],\n",
    ")\n",
    "# Apply the normalization\n",
    "image_tensor = normalize(image_tensor)\n",
    "# Add a batch dimension\n",
    "input_batch = image_tensor.unsqueeze(0)\n",
    "\n",
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "img_model.to(device)\n",
    "input_batch = input_batch.to(device)\n",
    "\n",
    "# Get the embeddings from the model\n",
    "with T.no_grad():\n",
    "    embeddings = img_model(input_batch).cpu()\n",
    "# Reshape the embeddings to a 1D tensor [2048,]\n",
    "img_embeddings = embeddings.squeeze()\n",
    "\n",
    "# Establish encoding pre-trained model\n",
    "sen_model = SentenceTransformer('sentence-transformers/all-roberta-large-v1')\n",
    "def get_embeddings(text):\n",
    "    embeddings = sen_model.encode(text)\n",
    "    return embeddings\n",
    "\n",
    "# Get files\n",
    "books = pickle.load(open('./build_dataset/book_dataset.pkl', 'rb'))\n",
    "slide_text = pickle.load(open('./build_dataset/book_summaries.pkl', 'rb'))\n",
    "slide_labels = pickle.load(open('./build_dataset/book_labels.pkl', 'rb'))\n",
    "slide_bboxs = pickle.load(open('./build_dataset/book_bbox.pkl', 'rb'))\n",
    "\n",
    "for b in books:\n",
    "    pkl_file = []\n",
    "    print(b)\n",
    "    # Get the papers (chapters)\n",
    "    chapters = books[b]\n",
    "    idd = 0\n",
    "    for chap in chapters:\n",
    "        chp_dict = {}\n",
    "        # Add id of the book\n",
    "        chp_dict['idd'] = idd\n",
    "\n",
    "        # PAPER PART \n",
    "        chp_dict['paper'] = {}\n",
    "        # Add the title of the chapter\n",
    "        chp_dict['paper']['title'] = {'text': b + \": \" + chap, 'embedding': get_embeddings(b + \": \" + chap)}\n",
    "        print(\"PAPER-chapter:\", b + \".\" + chap)\n",
    "        # Treat the paragraphs as sections\n",
    "        sections = chapters[chap]\n",
    "        sec_list = []\n",
    "        for paragraphs in sections:\n",
    "            sentences = []\n",
    "            for sentence in paragraphs:\n",
    "                sen_dict = {'text': sentence, 'embedding': get_embeddings(sentence)}\n",
    "                sentences.append(sen_dict)\n",
    "            sec_list.append(sentences)\n",
    "            \n",
    "        chp_dict['paper']['sections'] = sec_list\n",
    "\n",
    "        # Put placeholder image [plain rectangular figure]\n",
    "        chp_dict['paper']['figures'] = [{'name': 'placeholder_img.png', 'caption': \"\", 'embedding':[], 'pixel': np.zeros((480, 480, 3), dtype=np.uint8), 'feature': img_embeddings.tolist()}]\n",
    "\n",
    "        # SLIDE PART - get summary sentences of current book from summaries ==============================\n",
    "        # Initialise\n",
    "        chp_dict['slide'] = {'pages': []}\n",
    "\n",
    "        section_texts = slide_text[b][chap]\n",
    "        section_labels = slide_labels[b][idd]\n",
    "        section_bboxs = slide_bboxs[b][chap]\n",
    "        # print(section_bboxs, section_labels, section_texts)\n",
    "        \n",
    "        for i in range(len(section_texts)):\n",
    "            # Initialise the chapter dictionary for this page \n",
    "            section_slides = {'page': [], 'figure': []}\n",
    "            # List of sections and their corresponding slides\n",
    "            object_dict = {'text': section_texts[i], 'embedding': get_embeddings(section_texts[i]), 'bbox': section_bboxs[i]['text'], 'label': section_labels[i]} # the last character of title refer to the placement of the sentence in the section \n",
    "            \n",
    "            # Fill section_slides - each section has one text object and one image object\n",
    "            section_slides['page'].append(object_dict)\n",
    "            section_slides['figure'].append({'name': 'placeholder_img.png', 'bbox': section_bboxs[i]['image'], 'label': 0})\n",
    "\n",
    "            # Add section slides to pages\n",
    "            chp_dict['slide']['pages'].append([section_slides])\n",
    "\n",
    "        # Add book to pkl file after extracting info\n",
    "        pkl_file.append(chp_dict)\n",
    "        idd += 1\n",
    "\n",
    "    # Create pkl file\n",
    "    with open(f'./books/{b}.pkl', 'wb') as file:\n",
    "        pickle.dump(pkl_file, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PROMPT GENERATION\n",
    "client = Client()\n",
    "\n",
    "def get_prompt(text, prev, model=\"gpt-4-turbo\"):\n",
    "    prompt = f\"\"\"\n",
    "            You are an excellent film maker who can imagine scenes based on given text from a book. \n",
    "            Your job is to describe me a picture of a scene in one English sentence based on the following text: {text}\n",
    "            and the context of the prior text: {prev}. Please focus more on the setting and actions present in the texts.\n",
    "\n",
    "            Here is an example:\n",
    "            text: \"I think,\" said I, following as far as I could the methods of my companion, \"that Dr. Mortimer is a successful, elderly medical man, well-esteemed since those who know him give him this mark of their appreciation\" \"Good!\" said Holmes. \"I think also that the probability is in favour of his being a country practitioner who does a great deal of his visiting on foot.\" \"Then I was right.\" \"To that extent.\" \"But that was all.\"\n",
    "            context: In a dimly lit room, Sherlock Holmes stands by the hearth-rug, examining a thick, bulbous-headed stick engraved with the words \\\"To James Mortimer, M.R.S., from his friends of the C.C.H., 1884,\\\"\n",
    "            output: 'In a dimly lit room, he is having a serious discussion with his companion whilst holding a bulbous-headed stick'\n",
    "\n",
    "            If the context is empty, please generate the output based from the text only.\n",
    "            You must strictly follow the desired output format (a single string).\n",
    "            Your output must be only strictly English and within 70 words.\n",
    "            \"\"\"\n",
    "    \n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANIMATEDIFF\n",
    "def generate_visuals_vid(pipe, prompt, chap_title, section):\n",
    "    # Create directory for gifs\n",
    "    gif_dir = f'./generated_test_slides/{chap_title}_gifs'\n",
    "    if not os.path.exists(gif_dir):\n",
    "        os.makedirs(gif_dir)\n",
    "    \n",
    "    output = pipe(\n",
    "        prompt=(prompt),\n",
    "        negative_prompt=\"bad quality, worse quality\",\n",
    "        num_frames=16,\n",
    "        guidance_scale=7.5,\n",
    "        num_inference_steps=25,\n",
    "        generator=T.Generator(device).manual_seed(42),\n",
    "    )\n",
    "    frames = output.frames[0]\n",
    "    export_to_gif(frames, f\"{gif_dir}/{chap_title}_scene_{section}.gif\")\n",
    "    return f\"{gif_dir}/{chap_title}_scene_{section}.gif\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STABLEDIFF\n",
    "def generate_visuals_img(pipe, prompt, chap_title, section):\n",
    "    png_dir = f'./generated_test_slides/{chap_title}_pngs'\n",
    "    if not os.path.exists(png_dir):\n",
    "        os.makedirs(png_dir)\n",
    "        \n",
    "    image = pipe(prompt).images[0]    \n",
    "    image.save(f\"{png_dir}/{chap_title}_scene_{section}.png\")\n",
    "    return f\"{png_dir}/{chap_title}_scene_{section}.png\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Models - Base DOC2PPT and Fine-tuned DOC2PPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = T.device(\"cuda\" if T.cuda.is_available() else \"cpu\")\n",
    "\n",
    "loaded_model = T.load('./models/model_hse-tf.pt', map_location=T.device('cpu'))\n",
    "loaded_loc_model = T.load('./models/ciou_locmodel_best_early.pt', map_location=T.device('cpu'))\n",
    "story_loaded_model = T.load('./models/storybook_model_dropout_best.pt', map_location=T.device('cpu'))\n",
    "story_loaded_loc_model = T.load('./models/ciou_storyloc_best_scratch.pt', map_location=T.device('cpu'))\n",
    "\n",
    "prefix_to_remove = 'module.'\n",
    "loc_model_state_dict = {key[len(prefix_to_remove):] if key.startswith(prefix_to_remove) else key: value for key, value in loaded_loc_model.items()}\n",
    "story_loc_model_state_dict = {key[len(prefix_to_remove):] if key.startswith(prefix_to_remove) else key: value for key, value in story_loaded_loc_model.items()}\n",
    "\n",
    "#  PIPELINE 1 + 2 - testing base DOC2PPT with scientific papers + novels\n",
    "# base model\n",
    "base_model = Model().to(device)\n",
    "base_model.load_state_dict(loaded_model)\n",
    "\n",
    "# base object placer\n",
    "base_loc_model = MLPlayout().to(device)\n",
    "base_loc_model.load_state_dict(loc_model_state_dict)\n",
    "\n",
    "\n",
    "# PIPELINE 3 + 4 - testing fine-tuned DOC2PPT with novels and scientific papers\n",
    "# story model\n",
    "story_model = Model().to(device)\n",
    "story_model.load_state_dict(story_loaded_model)\n",
    "\n",
    "# story object placer\n",
    "story_loc_model = MLPlayout().to(device)\n",
    "story_loc_model.load_state_dict(story_loc_model_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_dat = {}\n",
    "stories_dat = {}\n",
    "\n",
    "for conf in json.load(open('./data/v1.0/train_val_test_2.json', 'r')): # train_val_test_2.json is the json file containing only 14 out of 19 conferences; train_val_test.json contains the full dataset\n",
    "   pkl = pickle.load(open(f'./data/v1.0/{conf}.pkl', 'rb'))\n",
    "   paper_dat[conf] = {}\n",
    "   for item in pkl:\n",
    "       idd = item['idd']\n",
    "       paper_dat[conf][idd] = item\n",
    "\n",
    "for conf in json.load(open('./data/v1.0/book_json.json', 'r')):\n",
    "    pkl = pickle.load(open(f'./books/{conf}.pkl', 'rb'))\n",
    "    stories_dat[conf] = {}\n",
    "    for item in pkl:\n",
    "        idd = item['idd']\n",
    "        stories_dat[conf][idd] = item\n",
    "    \n",
    "# Dataloader\n",
    "sci_papers = DLoader(paper_dat, 'test', domain=\"original\")\n",
    "stories = DLoader(stories_dat, 'test', domain=\"stories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Desired Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pad_tensor(vec, pad, dim):\n",
    "        pad_size = list(vec.shape)\n",
    "        pad_size[dim] = pad - vec.size(dim)\n",
    "        return T.cat([vec, T.zeros(*pad_size, device=device)], dim=dim)\n",
    "\n",
    "def get_objects_labels(data, model, loc_model, sec_slide_count=False):\n",
    "    outputs = {}\n",
    "    count = 0\n",
    "    for book in data:\n",
    "        out = model(book)\n",
    "        outputs[book['conf'] + \".\" + str(book['idd'])] = out\n",
    "        count += 1\n",
    "\n",
    "    # Get labels of desired objects\n",
    "    book_labels = {}\n",
    "    for book_out in outputs:\n",
    "        book_labels[book_out] = []\n",
    "        book_objs = outputs[book_out]['pd_obj']\n",
    "        if sec_slide_count:\n",
    "            num_sec_slides = outputs[book_out]['out_tok_page'] # Needed to get number of slides per each section (for evaluations)\n",
    "    \n",
    "            pred_sec_slide_num = [len(sec[:-1]) for sec in num_sec_slides] # Don't include action '1' as that indicates the end of the slide, not the slide itself\n",
    "            with open(f'./generated_test_slides/slide_section_counts/{book_out}_sec_counts.pkl', 'wb') as file: # Save to pickle files\n",
    "                pickle.dump(pred_sec_slide_num, file)\n",
    "\n",
    "        for section_objs in book_objs:\n",
    "            section_labels = []\n",
    "\n",
    "            for slide_objs in section_objs:\n",
    "                slide = {}\n",
    "                slide_labels = []\n",
    "\n",
    "                for obj in slide_objs[:-1]:\n",
    "                    label = T.argmax(obj)\n",
    "                    slide_labels.append(label.item())\n",
    "                slide['slide_objs'] = slide_labels\n",
    "\n",
    "                # Calculate bboxes\n",
    "                if slide_objs[:-1] != []:\n",
    "                    pd = T.cat(slide_objs[:-1], dim=0)\n",
    "                    pd_padded = [pad_tensor(i, 1024, 0) for i in pd]\n",
    "                    pd_bbox = [loc_model(p).cpu().detach().numpy() for p in pd_padded]\n",
    "\n",
    "                slide['locations'] = pd_bbox\n",
    "\n",
    "                section_labels.append(slide)\n",
    "            book_labels[book_out].append(section_labels)\n",
    "    return book_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "science_labels = get_objects_labels(sci_papers, base_model, base_loc_model)\n",
    "story_on_base_labels = get_objects_labels(stories, base_model, base_loc_model)\n",
    "story_labels = get_objects_labels(stories, story_model, story_loc_model, sec_slide_count=True)\n",
    "science_on_story_labels = get_objects_labels(sci_papers, story_model, story_loc_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PROGRESS TRACKER: Object Selection Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_accuracies(data, pred, sample=50):\n",
    "    pred_labels = []\n",
    "    for paper in pred:\n",
    "        sec_labels = []\n",
    "        for sections in pred[paper]:\n",
    "            slide_labels = []\n",
    "            for slides in sections:\n",
    "                objs_labels = []\n",
    "                for obj in slides['slide_objs']:\n",
    "                    label = obj\n",
    "                    objs_labels.append(label)\n",
    "                slide_labels.append(objs_labels)\n",
    "            sec_labels.append(slide_labels)\n",
    "        pred_labels.append(sec_labels)\n",
    "\n",
    "    count = 0\n",
    "    means = []\n",
    "    for e,f in zip(data, pred_labels):  \n",
    "        if count > sample:\n",
    "            break\n",
    "        dat = e['out_obj']\n",
    "\n",
    "        # Extra for-loop due to scientific papers having more than one slides for some sections\n",
    "        cleaned_data = [[[elem for elem in subsublist if elem is not None] for subsublist in sublist if subsublist != []] for sublist in dat if sublist != [] ]\n",
    "        gt = sum(cleaned_data, [])\n",
    "        gt = np.array(sum(gt, []))\n",
    "\n",
    "        # Predicted\n",
    "        pd = sum(f, [])\n",
    "        pd = np.array(sum(pd, []))\n",
    "\n",
    "        means.append(np.mean(gt == pd))\n",
    "        count += 1\n",
    "    return means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score for model 1: 0.6602599330876657\n",
      "accuracy score for model 2: 0.08723724532548062\n",
      "accuracy score for model 3: 0.7009956546721254\n",
      "accuracy score for model 4: 0.3815357351936311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3373802/1026473284.py:32: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  means.append(np.mean(gt == pd))\n"
     ]
    }
   ],
   "source": [
    "variants = [(sci_papers, science_labels), (stories, story_on_base_labels), (stories, story_labels), (sci_papers, science_on_story_labels)]\n",
    "model = [\"model 1\", \"model 2\", \"model 3\", \"model 4\"]\n",
    "for m, v in zip(model, variants):\n",
    "    print(f\"accuracy score for {m}:\", np.mean(get_label_accuracies(v[0], v[1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OBJECT PLACER: Bounding Box Prediction Performance (Mean IoU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bboxs(data, pred, type=\"paper\"):\n",
    "    pred_location = []\n",
    "    for paper in pred:\n",
    "        sec_location = []\n",
    "        for sections in pred[paper]:\n",
    "            slide_location = []\n",
    "            for slides in sections:\n",
    "                objs_location = []\n",
    "                for obj in slides['locations']:\n",
    "                    label = obj\n",
    "                    objs_location.append(label)\n",
    "                slide_location.append(objs_location)\n",
    "            sec_location.append(slide_location)\n",
    "        pred_location.append(sec_location)\n",
    "    \n",
    "    count = 0\n",
    "    pd_bbox = []\n",
    "    gd_bbox = []\n",
    "    slide_sizes = []\n",
    "    for e,f in zip(data, pred_location):\n",
    "        # Ground truth    \n",
    "        dat = e['out_bbox']\n",
    "\n",
    "        # Extra for-loop due to scientific papers having more than one slides for some sections\n",
    "        cleaned_data = [[[elem for elem in subsublist if elem is not None] for subsublist in sublist if subsublist != []] for sublist in dat if sublist != [] ]\n",
    "        gt = sum(cleaned_data, [])\n",
    "        gt = np.array(sum(gt, []))\n",
    "\n",
    "        # Predicted\n",
    "        pd = sum(f, [])\n",
    "        pd = np.array(sum(pd, []))\n",
    "\n",
    "        pd_bbox.append(pd)\n",
    "        gd_bbox.append(gt)\n",
    "        if type == \"paper\":\n",
    "            slide_sizes.append(e['slide_size'])\n",
    "        else:\n",
    "            slide_sizes.append([960, 720])\n",
    "\n",
    "        count += 1\n",
    "    return pd_bbox, gd_bbox, slide_sizes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_base, gd_base, sizes_base = get_bboxs(sci_papers, science_labels)\n",
    "pd_mix, gd_mix, sizes_mix = get_bboxs(stories, story_on_base_labels, type=\"story\")\n",
    "pd_story, gd_story, sizes_story = get_bboxs(stories, story_labels, type=\"story\")\n",
    "pd_mix_, gd_mix_, sizes_mix_ = get_bboxs(sci_papers, science_on_story_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [left, top, width, height] -> [x1, y1, x2, y2]\n",
    "def convert_bbox_format(bbox, slide_size):\n",
    "    slide_width, slide_height = slide_size\n",
    "\n",
    "    left, top, width, height = bbox\n",
    "    x1 = left * slide_width\n",
    "    y1 = top * slide_height\n",
    "    x2 = (left + width) * slide_width\n",
    "    y2 = (top + height) * slide_height\n",
    "    \n",
    "    return [x1, y1, x2, y2]\n",
    "\n",
    "def calculate_iou(box1, box2, slide_size):\n",
    "    # [left, top, width, height] -> [x1, y1, x2, y2]\n",
    "    box1 = convert_bbox_format(box1, slide_size) \n",
    "    box2 = convert_bbox_format(box2, slide_size) \n",
    "\n",
    "    # Calculate intersection coordinates\n",
    "    x1_inter = max(box1[0], box2[0])\n",
    "    y1_inter = max(box1[1], box2[1])\n",
    "    x2_inter = min(box1[2], box2[2])\n",
    "    y2_inter = min(box1[3], box2[3])\n",
    "    \n",
    "    # Calculate intersection area\n",
    "    inter_width = max(0, x2_inter - x1_inter)\n",
    "    inter_height = max(0, y2_inter - y1_inter)\n",
    "    inter_area = inter_width * inter_height\n",
    "    \n",
    "    # Calculate area of both bounding boxes\n",
    "    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
    "    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
    "    \n",
    "    # Calculate union area\n",
    "    union_area = box1_area + box2_area - inter_area\n",
    "    \n",
    "    # Calculate IoU\n",
    "    iou = inter_area / union_area if union_area > 0 else 0\n",
    "    return iou\n",
    "\n",
    "# Averages IOU scores of slide objects within slide deck\n",
    "def mean_iou(pred_boxes, gt_boxes, slide_size):\n",
    "    ious = []\n",
    "    for pred_box, gt_box in zip(pred_boxes, gt_boxes):\n",
    "        iou = calculate_iou(pred_box, gt_box, slide_size)\n",
    "        ious.append(iou)\n",
    "    \n",
    "    return np.mean(ious)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MIOU score for model 1: 0.09829466291333415\n",
      "MIOU score for model 2: 0.22393091893337758\n",
      "MIOU score for model 3: 0.33465671285270704\n",
      "MIOU score for model 4: 0.07724153442317817\n"
     ]
    }
   ],
   "source": [
    "variants = [(sci_papers, science_labels, \"paper\"), (stories, story_on_base_labels, \"stories\"), (stories, story_labels, \"stories\"), (sci_papers, science_on_story_labels, \"paper\")]\n",
    "model = [\"model 1\", \"model 2\", \"model 3\", \"model 4\"]\n",
    "\n",
    "for m, v in zip(model, variants):\n",
    "    pd_base, gd_base, sizes = get_bboxs(v[0], v[1], type=v[2])\n",
    "    means = []\n",
    "    for pred_boxes, gt_boxes, slide_size in zip(pd_base, gd_base, sizes):\n",
    "        mean_iou_score = mean_iou(pred_boxes, gt_boxes, slide_size)\n",
    "        means.append(mean_iou_score)\n",
    "    print(f\"MIOU score for {m}:\", np.mean(means))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Presentation making for Two Different Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Presentation Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_presentation_content(data, book_labels):\n",
    "    presentation_content = {}\n",
    "\n",
    "    for book, book_label in zip(data, book_labels):\n",
    "        input_text = book['inp_text']\n",
    "        section_labels = book_labels[book_label]\n",
    "\n",
    "        book_presentation = []\n",
    "        chap_num = 1\n",
    "        for section, sec_label in zip(input_text, section_labels):\n",
    "            if section is None:\n",
    "                continue\n",
    "            # print(f\"================== CHAPTER {chap_num} =====================\")\n",
    "            section_slides = []\n",
    "\n",
    "            # Concat input text with input figures to create singular list of possible slide objects\n",
    "            section = section + book['inp_fig']\n",
    "\n",
    "            slide_count = 1\n",
    "            for slide_labels in sec_label:\n",
    "                # print(f\"SLIDE {slide_count}\")\n",
    "                slide_sentences = []\n",
    "                for label, bbox in zip(slide_labels['slide_objs'], slide_labels['locations']):\n",
    "                    object_ = section[label]\n",
    "\n",
    "                    if any(object_.lower().endswith(ext) for ext in ['.jpg', '.jpeg', '.png', '.gif', '.bmp', '.tiff', '.webp']):\n",
    "                        if object_ == 'placeholder_img.png': \n",
    "                            chosen_obj = {'figure': object_, 'bbox': bbox}\n",
    "                        else:\n",
    "                            # Find pixels of figure in source doc\n",
    "                            fig = book['inp_fig'].index(object_)\n",
    "                            chosen_obj = {'figure': book['inp_pix'][fig], 'bbox': bbox}\n",
    "                    else:    \n",
    "                        chosen_obj = {'text': object_, 'bbox': bbox}\n",
    "                    slide_sentences.append(chosen_obj)\n",
    "                slide_count += 1\n",
    "                section_slides.append(slide_sentences)\n",
    "            chap_num += 1\n",
    "            book_presentation.append(section_slides)\n",
    "        presentation_content[book['conf'] + \".\" + str(book['idd'])] = book_presentation\n",
    "    return presentation_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_content = get_presentation_content(stories, story_labels)\n",
    "story_base_content = get_presentation_content(stories, story_on_base_labels)\n",
    "sci_paper_content = get_presentation_content(sci_papers, science_labels)\n",
    "sci_paper_story_content = get_presentation_content(sci_papers, science_on_story_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make BOOK2PPT Presentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_prompts(chapter):\n",
    "    generated_text = []\n",
    "   \n",
    "    for section in chapter:\n",
    "        for slide in section: # each section only has one slide with two objects (text and image)\n",
    "            slide_text = {}\n",
    "            for object_ in slide: \n",
    "                if 'text' in object_:\n",
    "                    current_text = object_['text']\n",
    "                    prev = generated_text[-1]['prompt'] if generated_text != [] else \"\" # get previous prompt for context; empty for the first generated prompt\n",
    "                    scene_prompt = get_prompt(current_text, prev)\n",
    "                    \n",
    "                    print(\"context:\", prev) \n",
    "                    print(\"prompt:\", scene_prompt)\n",
    "                    print()\n",
    "                    \n",
    "                    slide_text['text'] = current_text\n",
    "                    slide_text['prompt'] = scene_prompt\n",
    "                    slide_text['context'] = prev\n",
    "                    generated_text.append(slide_text)\n",
    "    return generated_text \n",
    "\n",
    "chosen_chapters = ['The Goose Girl.5', 'Wuthering Heights.0', 'Frankenstein.volume 1.2']\n",
    "\n",
    "# Create test slide decks for evaluation\n",
    "for book_src in chosen_chapters: \n",
    "    prompts = create_prompts(story_content[book_src]) \n",
    "    with open(f'./generated_test_slides/{book_src}_text_prompts.pkl', 'wb') as file:\n",
    "        pickle.dump(pkl_file, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_slides(prs, chp, source, layout=\"predicted\", generate_images=False, prompts=None, vis_generator=\"\"):\n",
    "    book = {chp: source[chp]}\n",
    "    \n",
    "    # Initialize the image generation pipeline\n",
    "    if vis_generator == \"animatediff\":\n",
    "        adapter = MotionAdapter.from_pretrained(\"guoyww/animatediff-motion-adapter-v1-5-2\", torch_dtype=T.float16)\n",
    "        model_id = \"SG161222/Realistic_Vision_V5.1_noVAE\"\n",
    "        pipe = AnimateDiffPipeline.from_pretrained(model_id, motion_adapter=adapter, torch_dtype=T.float16)\n",
    "        scheduler = DDIMScheduler.from_pretrained(model_id, subfolder=\"scheduler\", clip_sample=False, timestep_spacing=\"linspace\", beta_schedule=\"linear\", steps_offset=1)\n",
    "        pipe.scheduler = scheduler\n",
    "        pipe.enable_vae_slicing()\n",
    "        pipe.enable_model_cpu_offload()\n",
    "    elif vis_generator == \"stablediff\":\n",
    "        pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", torch_dtype=T.float16).to(\"cuda\")\n",
    "\n",
    "    prompts_used = [0]\n",
    "    \n",
    "    for chp, sections in book.items():\n",
    "        for sect in sections:\n",
    "            for slide_ in sect:\n",
    "                slide = prs.slides.add_slide(prs.slide_layouts[6])\n",
    "\n",
    "                # Define four possible layouts for ground truth bounding boxes\n",
    "                if layout == \"ground_truth\":\n",
    "                    predefined_layouts = [{\"text\": [0.25, 5.25, 9.5, 1], \"image\": [2.5, 0.1, 5, 5]}, {\"text\": [0.25, 0.1, 9.5, 1], \"image\": [2.5, 2.25, 5, 5]}, {\"text\": [5.25, 1, 4.5, 5], \"image\": [0.1, 1, 5, 5]}, {\"text\": [0.1, 1, 4.5, 5], \"image\": [4.85, 1, 5, 5]}]\n",
    "                    layout_idx = random.choice([0, 1, 2, 3])\n",
    "                    t_left, t_top, t_width, t_height = predefined_layouts[layout_idx]['text']\n",
    "                    i_left, i_top, i_width, i_height = predefined_layouts[layout_idx]['image']\n",
    "                else:\n",
    "                    t_left = t_top = t_width = t_height = i_left = i_top = i_width = i_height = 0\n",
    "                \n",
    "                # For each chosen object in slide\n",
    "                for obj in slide_:\n",
    "                    # Define which layout to use: ground-truth or predicted; if former get a random predefined layout, Otherwises, get predicted bbox of current slide object\n",
    "                    if layout == \"predicted\":\n",
    "                        left, top, width, height = obj['bbox']\n",
    "                        left = max(0, left * prs.slide_width) / 914400\n",
    "                        top = max(0, top * prs.slide_height) / 914400\n",
    "                        width = max(0, width * prs.slide_width) / 914400\n",
    "                        height = max(0, height * prs.slide_height) / 914400\n",
    "                    else:\n",
    "                        left, top, width, height = (i_left, i_top, i_width, i_height) if 'figure' in obj else (t_left, t_top, t_width, t_height)\n",
    "                    \n",
    "                    if 'text' in obj:\n",
    "                        # Paraphrase the text if it has 70 tokens or more \n",
    "                        text_box = slide.shapes.add_textbox(left=Inches(left), top=Inches(top), width=Inches(width), height=Inches(height))\n",
    "                        text_frame = text_box.text_frame\n",
    "                        if len(obj['text']) > 70:\n",
    "                            text = summarizer(obj['text'], max_length=150, min_length=30, do_sample=False)[0]['summary_text']\n",
    "                        else:\n",
    "                            text = obj['text']\n",
    "                        #  Ensures that the text fits within the confines of the bounding box\n",
    "                        wrap_text(text=text, max_width=Inches(width), text_frame=text_frame)\n",
    "                    \n",
    "                    elif 'figure' in obj:\n",
    "                        # If chosen figure states name of placeholder image, generate visuals (or placeholder rectangles for groundtruth slide deck creation)\n",
    "                        if isinstance(obj['figure'], str):\n",
    "                            if generate_images:\n",
    "                                counter = prompts_used[-1]\n",
    "                                scene_prompt = prompts[counter]['prompt'] if \"can I help you today?\" and \"Join our discord for more:\" not in prompts[counter]['prompt'] else prompts[counter]['context']\n",
    "                                if vis_generator == \"animatediff\":\n",
    "                                    image = generate_visuals_vid(pipe, scene_prompt, chap_title=chp, section=counter)\n",
    "                                    prompts_used.append(counter + 1)\n",
    "                                else:\n",
    "                                    image = generate_visuals_img(pipe, scene_prompt, chap_title=chp, section=counter)\n",
    "                                    prompts_used.append(counter + 1)\n",
    "\n",
    "                                slide.shapes.add_picture(image, left=Inches(left), top=Inches(top), width=Inches(width), height=Inches(height))\n",
    "                            else:\n",
    "                                slide.shapes.add_shape(MSO_SHAPE.RECTANGLE, left=Inches(left), top=Inches(top), width=Inches(width), height=Inches(height))\n",
    "\n",
    "                        else:\n",
    "                            image = Image.fromarray(obj['figure'])\n",
    "                            image_path = \"./temp_image.png\"\n",
    "                            image.save(image_path)\n",
    "                            \n",
    "                            slide.shapes.add_picture(image_path, left=Inches(left), top=Inches(top), width=Inches(width), height=Inches(height))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate different variations of the model\n",
    "variants = ['story', 'story_on_base', 'sci_paper', 'sci_paper_on_story'] \n",
    "content = [story_content, story_base_content, sci_paper_content, sci_paper_story_content]\n",
    "\n",
    "for v, type_ in zip(variants[2:3], content[2:3]):\n",
    "    # Get sample chapters and turn them into a smaller dictionary\n",
    "    chosen_chapters = random.sample(list(type_.items()), 1)\n",
    "    for book_src in chosen_chapters:\n",
    "        prs = Presentation()\n",
    "        if \"story\" in v:\n",
    "            create_slides(prs=prs, chp=book_src[0], source=type_, layout=\"predicted\", generate_images=False)\n",
    "        elif \"sci\" in v:\n",
    "            create_slides(prs=prs, chp='cvpr10.29', source=type_, layout=\"predicted\")\n",
    "        prs.save(f'{v}.pptx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chosen_chapters = ['The Goose Girl.5', 'Wuthering Heights.0', 'Frankenstein.volume 1.2']\n",
    "\n",
    "# Create test slide decks for evaluation\n",
    "for book_src in chosen_chapters:\n",
    "    print(book_src)\n",
    "    prs_1 = Presentation()\n",
    "    prs_2 = Presentation()\n",
    "    \n",
    "    prompts = pickle.load(open(f'./generated_test_slides/{book_src}_text_prompts.pkl', 'rb'))\n",
    "    \n",
    "    create_slides(prs=prs_1, chp=book_src, source=story_content, layout=\"ground_truth\", generate_images=True, prompts=prompts, vis_generator=\"stablediff\")\n",
    "    prs_1.save(f'./generated_test_slides/{book_src}_stablediff.pptx')\n",
    "    \n",
    "    create_slides(prs=prs_2, chp=book_src, source=story_content, layout=\"ground_truth\", generate_images=True, prompts=prompts, vis_generator=\"animatediff\")\n",
    "    prs_2.save(f'./generated_test_slides/{book_src}_animatediff.pptx')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
